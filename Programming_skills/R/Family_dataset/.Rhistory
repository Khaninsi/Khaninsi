knitr::opts_chunk$set(echo = TRUE)
load("/Users/khaninsi/Documents/Github/Khaninsi/Programming_skills/R/data/family.rda")
library(MASS)
library(ISLR2)
library(car)
class(cars) # Show the class
head(cars)  # Show top 6 rows
dim(cars)  # Dimensions
summary(cars)
# Scatter plot
plot(cars, xlab = "Speed (mph)", ylab = "Stopping distance (ft)",
las = 1)
# Speed (mph) and Stopping distance (ft) seem to have a linear relationship
## Plot histogram to understand distribution of the data
plot(density(cars$speed)) # Speed has a normal distribution with mean around 15 mph.
plot(density(cars$dist)) # Distance has a normal distribution with positive skewness.
## Calculate correlation for all variables in R to understand linear relationship
cor(cars)
# Speed and distance have a very high correlation with around 0.8, which
#indicates a strong linear relationship.
knitr::include_graphics("/Users/khaninsi/Documents/USC Master/Spring 2022/ISE 529 Predictive Analytics/Homework/HW4/images/linear_pop.png")
knitr::include_graphics("/Users/khaninsi/Documents/USC Master/Spring 2022/ISE 529 Predictive Analytics/Homework/HW4/images/linear_samp.png")
library(MASS)
library(ISLR2)
library(car)
class(cars) # Show the class
head(cars)  # Show top 6 rows
dim(cars)  # Dimensions
summary(cars)
# Scatter plot
plot(cars, xlab = "Speed (mph)", ylab = "Stopping distance (ft)",
las = 1)
# Speed (mph) and Stopping distance (ft) seem to have a linear relationship
## Plot histogram to understand distribution of the data
plot(density(cars$speed)) # Speed has a normal distribution with mean around 15 mph.
plot(density(cars$dist)) # Distance has a normal distribution with positive skewness.
## Calculate correlation for all variables in R to understand linear relationship
cor(cars)
# Speed and distance have a very high correlation with around 0.8, which
#indicates a strong linear relationship.
knitr::include_graphics("/Users/khaninsi/Documents/Github/Khaninsi/Programming_skills/R/Linear_regression/images/linear_pop.png")
knitr::include_graphics("/Users/khaninsi/Documents/Github/Khaninsi/Programming_skills/R/Linear_regression/images/linear_samp.png")
lm.fit =lm(dist~speed, data=cars) # fit linear regression on speed(X) with dist(Y)
summary(lm.fit)
set.seed(1)
# Create a vector x in R that has 30 values going from 1 to 4 by 1/3 three times
x = rep(seq(1,length=10,by=1/3), times=3)
y = 5 + x + 4* x^2 # create a variable y as follows: 5 + x + 4*x^2
y = y + rnorm(30, sd = 9) # a vector of noise drawn from N(0,9) to y
plot(x)
plot(y)
lm.fit =lm(y~x) # fit linear regression on y with x
summary(lm.fit)
# Plot residuals
plot(fitted(lm.fit), resid(lm.fit))
abline(0,0)
plot(x, y)
abline(lm.fit, lwd = 3,col = 'red')
knitr::opts_chunk$set(echo = TRUE)
# The setup chunk is run automatically before any other code to make sure package requirements are satisfied. #it's an example of how to preload packages.
# library(tidyverse)
# library(ggplot2)
# Add more packages here
library(tree)
knitr::include_graphics("/Users/khaninsi/Documents/Github/Khaninsi/Programming skills/R/Tree-based_models/LogisticRegression.png")
library(caret)
library(gbm)
library(stats)
library(pROC)
set.seed(1)
boost.titanic <- gbm(as.integer(survived) - 1 ~ ., data = titanic[train, ],
shrinkage = .02, distribution = 'bernoulli', n.trees = 5000,
verbose = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# The setup chunk is run automatically before any other code to make sure package requirements are satisfied
# library(tidyverse)
# library(ggplot2)
# Add more packages here
# library(nnet)
# library(class)
# library(MASS)
## Loading the data
flea = read.csv(file = "flea.csv", header = TRUE)
head(flea) # Print the top 6 rows
dim(flea)  # Print dimensions of the data
table(flea$species) # Count distinct class
summary(flea[, c(-1)]) # Understand how the data behave
# Observation of flea:
# No missing values, data are all numerical, and no obvious outliers
# Since we have a very small sample for each class (21 22 31), I choose to not split the data
# into training and test dataset. Therefore, I would compare the accuracy of these methods using
# training accuracy and errors.
# Normally, the Logistic Regression in glm cannot handle multiple classes.
# Thus, I will separate the data into 3 datasets as the following.
flea.1 = flea
flea.1[flea.1$species != 1,]$species = 0
flea.2 = flea
flea.2[flea.2$species != 2,]$species = 0
flea.3 = flea
flea.3[flea.3$species != 3,]$species = 0
# Each dataset contains only its class and 0
knitr::opts_chunk$set(echo = TRUE)
# The setup chunk is run automatically before any other code to make sure package requirements are satisfied
# library(tidyverse)
# library(ggplot2)
# Add more packages here
# library(nnet)
# library(class)
# library(MASS)
knitr::include_graphics("Logistic_equations.png")
